\chapter{Definizioni}\label{ch:chapter1}
In questo capitolo andiamo a introdurre i concetti principali su cui si basa l'elaborato, che ci serviranno successivamente per apprezzarne l'utilità. 
\section{Continual Learning}
Negli ultimi anni i modelli del \textbf{Machine Learning} sono stati addirittura in grado di sorpassare l'intelletto umano per svariati problemi, come ad esempio il \textbf{Visual Recognition}.
Sebbene questi risultati siano sorprendenti, sono stati ottenuti con modelli statici non in grado di espandere il loro comportamento o adattarlo nel tempo. Si introduce quindi il concetto del \textbf{Continual Learning} su cui si basa questo elaborato.\newline
\textit{Continual Learning} mira a creare algoritmi di\textit{Machine Learning} in grado di accumulare un insieme di conoscenze apprese sequenzialmente. L'idea generale alla base di quest'ultimo è rendere gli algoritmi in grado di apprendere da una fonte di dati reale. In un ambiente naturale le opportunità di apprendimento non sono disponibili contemporaneamente e devono essere elaborate in sequenza.\newline
Il \textbf{Continual Learning} studia il problema dell'apprendimento da uno \textit{stream} infinito di dati, con l'obbiettivo di estendere gradualmente la conoscenza e di usarla per allenamenti successivi. La dimensione dello \textit{stream} di dati ed il numero di \textit{tasks} su cui si lavora non è necessariamente noto a priori.  Il \textit{Continual Learning} può essere anche definito come \textbf{Lifelong Learning}, \textbf{Sequential Learning} o \textbf{Incremental Learning}.
Il concetto fondamentale su cui si basa  è la natura sequenziale del processo di apprendimento, in cui solo una porzione dei dati di input di uno o più \textit{tasks} è disponibile in quell'istante di tempo.
\section{Catastrophic Forgetting}
Come viene introdotto in \cite{Continual_Learning}, una rete neurale \textit{dimentica} quando le sue prestazioni su una distribuzione dati vengono ridotte dall'apprendimento su una successiva.
La sfida maggiore del \textit{Continual Learning} consiste nell'apprendere evitando il  \textbf{Catastrophic Forgetting}, cioè la performance di previsione su dati appartenenti a \textit{tasks} precedentemente visionati nel training non dovrebbe calare nel tempo in seguito all'aggiunta di nuovi \textit{tasks}.
Definiamo adesso il \textbf{Catastriphic Forgetting}, noto anche come \textbf{Catastrophic Interference}, come la tendenza di una rete neurale artificiale a dimenticare completamente e in modo improvviso le informazioni apprese in precedenza dopo aver appreso nuove informazioni (come viene definito in \cite{Continual_Learning}).
\section{Stability-Plasticity Dilemma}
Per sopperire al \textbf{Catastrophic Forgetting} i sistemi di apprendimento devono, da una parte , mostrare la capacità di acquisire nuova conoscenza e affinare quella già esistente sulla base di un input continuativo, dall'altra, impedire alla nuova informazione di interferire con la conoscenza pregressa.
Il concetto  per il quale un sistema sia in grado di essere "\textit{plastico}" per l'integrazione di nuove informazioni e "\textit{stabile}"
in modo da non interferire \textit{catastroficamente} con la conoscenza precedentemente consolidata è definito con il nome \textbf{Stability-Plasticity Dilemma}.\newline
Come viene descritto in \cite{stability/plasticity} , troppa "plasticità" farà sì che i dati precedentemente codificati siano costantemente dimenticati, mentre troppa "stabilità" impedirà la codifica efficiente di questi dati a livello delle sinapsi.
\section{Task Agnostic-Task Aware}
Un concetto importante su cui si basa l'esecuzione del mio programma che simula il \textbf{Continual Learning} è la dualità di approccio \textbf{Task-Agnostic/Task-Aware}.
Definiamo un approccio \textbf{Task-Agnostic} quando la rete non conosce bene i limiti dei \textit{tasks}, cioè non conosciamo a quale \textit{task} appartenga la \textit{label} del dato in input alla \textbf{Rete Neurale Convoluzionale}. Mentre, nell'altro caso, l'approccio  \textbf{Task-Agnostic} è contraddistinto dalla nozione del \textit{task} corrente a cui appartiene il dato in input.\newline
Questi due approcci ci consentono di ottenere risultati diversi sotto l'aspetto dell'\textit{accuracy} della rete, sia durante che al termine del ciclo previsto per vedere tutti i \textit{tasks}. In particolar modo noi possiamo avere per entrambi sia il processo di \textbf{training} che quello di \textbf{testing}, ottenendo sostanzialmente quattro combinazioni di possibili processi.\newline
Vedremo successivamente come sarà possibile simulare queste due tipologie di \textit{training/testing} tramite il metodo \textit{set\_tasks} della classe che rappresenta la 
\textbf{rete neurale convoluzionale}.
